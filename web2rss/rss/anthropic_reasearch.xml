<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>anthropic_reasearch</title>
    <link>https://www.anthropic.com/research</link>
    <description>Latest posts from https://www.anthropic.com/research. follow.is: feedId:74838602499110912+userId:73721007538233344</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 06 Jan 2025 02:40:07 +0000</lastBuildDate>
    <item>
      <title>Predictability and Surprise in Large Generative Models</title>
      <link>https://www.anthropic.com/research/predictability-and-surprise-in-large-generative-models</link>
      <description>Societal ImpactsPredictability and Surprise in Large Generative Models2022年2月15日</description>
    </item>
    <item>
      <title>Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</title>
      <link>https://www.anthropic.com/research/red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned</link>
      <description>Societal ImpactsRed Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned2022年8月22日</description>
    </item>
    <item>
      <title>The Capacity for Moral Self-Correction in Large Language Models</title>
      <link>https://www.anthropic.com/research/the-capacity-for-moral-self-correction-in-large-language-models</link>
      <description>Societal ImpactsThe Capacity for Moral Self-Correction in Large Language Models2023年2月15日</description>
    </item>
    <item>
      <title>Towards Measuring the Representation of Subjective Global Opinions in Language Models</title>
      <link>https://www.anthropic.com/research/towards-measuring-the-representation-of-subjective-global-opinions-in-language-models</link>
      <description>Societal ImpactsTowards Measuring the Representation of Subjective Global Opinions in Language Models2023年6月29日</description>
    </item>
    <item>
      <title>Challenges in evaluating AI systems</title>
      <link>https://www.anthropic.com/research/evaluating-ai-systems</link>
      <description>PolicyChallenges in evaluating AI systems2023年10月4日</description>
    </item>
    <item>
      <title>Collective Constitutional AI: Aligning a Language Model with Public Input</title>
      <link>https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input</link>
      <description>Policy·  Societal ImpactsCollective Constitutional AI: Aligning a Language Model with Public Input2023年10月17日</description>
    </item>
    <item>
      <title>Evaluating and Mitigating Discrimination in Language Model Decisions</title>
      <link>https://www.anthropic.com/research/evaluating-and-mitigating-discrimination-in-language-model-decisions</link>
      <description>Societal ImpactsEvaluating and Mitigating Discrimination in Language Model Decisions2023年12月8日</description>
    </item>
    <item>
      <title>Measuring the Persuasiveness of Language Models</title>
      <link>https://www.anthropic.com/research/measuring-model-persuasiveness</link>
      <description>Societal ImpactsMeasuring the Persuasiveness of Language Models2024年4月9日</description>
    </item>
    <item>
      <title>Testing and mitigating elections-related risks</title>
      <link>https://www.anthropic.com/news/testing-and-mitigating-elections-related-risks</link>
      <description>Policy·  Societal ImpactsTesting and mitigating elections-related risks2024年6月6日</description>
    </item>
    <item>
      <title>Evaluating feature steering: A case study in mitigating social biases</title>
      <link>https://www.anthropic.com/research/evaluating-feature-steering</link>
      <description>Societal Impacts·  InterpretabilityEvaluating feature steering: A case study in mitigating social biases2024年10月25日</description>
    </item>
    <item>
      <title>Clio: A system for privacy-preserving insights into real-world AI use</title>
      <link>https://www.anthropic.com/research/clio</link>
      <description>Societal ImpactsClio: A system for privacy-preserving insights into real-world AI use2024年12月12日</description>
    </item>
    <item>
      <title>A General Language Assistant as a Laboratory for Alignment</title>
      <link>https://www.anthropic.com/research/a-general-language-assistant-as-a-laboratory-for-alignment</link>
      <description>AlignmentA General Language Assistant as a Laboratory for Alignment2021年12月1日</description>
    </item>
    <item>
      <title>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</title>
      <link>https://www.anthropic.com/research/training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback</link>
      <description>AlignmentTraining a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback2022年4月12日</description>
    </item>
    <item>
      <title>Language Models (Mostly) Know What They Know</title>
      <link>https://www.anthropic.com/research/language-models-mostly-know-what-they-know</link>
      <description>AlignmentLanguage Models (Mostly) Know What They Know2022年7月11日</description>
    </item>
    <item>
      <title>Measuring Progress on Scalable Oversight for Large Language Models</title>
      <link>https://www.anthropic.com/research/measuring-progress-on-scalable-oversight-for-large-language-models</link>
      <description>AlignmentMeasuring Progress on Scalable Oversight for Large Language Models2022年11月4日</description>
    </item>
    <item>
      <title>Constitutional AI: Harmlessness from AI Feedback</title>
      <link>https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback</link>
      <description>AlignmentConstitutional AI: Harmlessness from AI Feedback2022年12月15日</description>
    </item>
    <item>
      <title>Discovering Language Model Behaviors with Model-Written Evaluations</title>
      <link>https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</link>
      <description>AlignmentDiscovering Language Model Behaviors with Model-Written Evaluations2022年12月19日</description>
    </item>
    <item>
      <title>Question Decomposition Improves the Faithfulness of Model-Generated Reasoning</title>
      <link>https://www.anthropic.com/research/question-decomposition-improves-the-faithfulness-of-model-generated-reasoning</link>
      <description>AlignmentQuestion Decomposition Improves the Faithfulness of Model-Generated Reasoning2023年7月18日</description>
    </item>
    <item>
      <title>Measuring Faithfulness in Chain-of-Thought Reasoning</title>
      <link>https://www.anthropic.com/research/measuring-faithfulness-in-chain-of-thought-reasoning</link>
      <description>AlignmentMeasuring Faithfulness in Chain-of-Thought Reasoning2023年7月18日</description>
    </item>
    <item>
      <title>Studying Large Language Model Generalization with Influence Functions</title>
      <link>https://www.anthropic.com/research/studying-large-language-model-generalization-with-influence-functions</link>
      <description>AlignmentStudying Large Language Model Generalization with Influence Functions2023年8月8日</description>
    </item>
    <item>
      <title>Tracing Model Outputs to the Training Data</title>
      <link>https://www.anthropic.com/research/influence-functions</link>
      <description>AlignmentTracing Model Outputs to the Training Data2023年8月8日</description>
    </item>
    <item>
      <title>Towards Understanding Sycophancy in Language Models</title>
      <link>https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models</link>
      <description>AlignmentTowards Understanding Sycophancy in Language Models2023年10月24日</description>
    </item>
    <item>
      <title>Specific versus General Principles for Constitutional AI</title>
      <link>https://www.anthropic.com/research/specific-versus-general-principles-for-constitutional-ai</link>
      <description>AlignmentSpecific versus General Principles for Constitutional AI2023年10月25日</description>
    </item>
    <item>
      <title>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</title>
      <link>https://www.anthropic.com/research/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training</link>
      <description>AlignmentSleeper Agents: Training Deceptive LLMs that Persist Through Safety Training2024年1月15日</description>
    </item>
    <item>
      <title>Many-shot jailbreaking</title>
      <link>https://www.anthropic.com/research/many-shot-jailbreaking</link>
      <description>AlignmentMany-shot jailbreaking2024年4月3日</description>
    </item>
    <item>
      <title>Simple probes can catch sleeper agents</title>
      <link>https://www.anthropic.com/research/probes-catch-sleeper-agents</link>
      <description>Alignment·  InterpretabilitySimple probes can catch sleeper agents2024年4月23日</description>
    </item>
    <item>
      <title>Claude’s Character</title>
      <link>https://www.anthropic.com/research/claude-character</link>
      <description>AlignmentClaude’s Character2024年6月9日</description>
    </item>
    <item>
      <title>Sycophancy to subterfuge: Investigating reward tampering in language models</title>
      <link>https://www.anthropic.com/research/reward-tampering</link>
      <description>AlignmentSycophancy to subterfuge: Investigating reward tampering in language models2024年6月17日</description>
    </item>
    <item>
      <title>Sabotage evaluations for frontier models</title>
      <link>https://www.anthropic.com/research/sabotage-evaluations</link>
      <description>AlignmentSabotage evaluations for frontier models2024年10月19日</description>
    </item>
    <item>
      <title>Alignment faking in large language models</title>
      <link>https://www.anthropic.com/research/alignment-faking</link>
      <description>AlignmentAlignment faking in large language models2024年12月18日</description>
    </item>
    <item>
      <title>A Mathematical Framework for Transformer Circuits</title>
      <link>https://www.anthropic.com/research/a-mathematical-framework-for-transformer-circuits</link>
      <description>InterpretabilityA Mathematical Framework for Transformer Circuits2021年12月22日</description>
    </item>
    <item>
      <title>In-context Learning and Induction Heads</title>
      <link>https://www.anthropic.com/research/in-context-learning-and-induction-heads</link>
      <description>InterpretabilityIn-context Learning and Induction Heads2022年3月8日</description>
    </item>
    <item>
      <title>Scaling Laws and Interpretability of Learning from Repeated Data</title>
      <link>https://www.anthropic.com/research/scaling-laws-and-interpretability-of-learning-from-repeated-data</link>
      <description>InterpretabilityScaling Laws and Interpretability of Learning from Repeated Data2022年5月21日</description>
    </item>
    <item>
      <title>Softmax Linear Units</title>
      <link>https://www.anthropic.com/research/softmax-linear-units</link>
      <description>InterpretabilitySoftmax Linear Units2022年6月17日</description>
    </item>
    <item>
      <title>Toy Models of Superposition</title>
      <link>https://www.anthropic.com/research/toy-models-of-superposition</link>
      <description>InterpretabilityToy Models of Superposition2022年9月14日</description>
    </item>
    <item>
      <title>Superposition, Memorization, and Double Descent</title>
      <link>https://www.anthropic.com/research/superposition-memorization-and-double-descent</link>
      <description>InterpretabilitySuperposition, Memorization, and Double Descent2023年1月5日</description>
    </item>
    <item>
      <title>Privileged Bases in the Transformer Residual Stream</title>
      <link>https://www.anthropic.com/research/privileged-bases-in-the-transformer-residual-stream</link>
      <description>InterpretabilityPrivileged Bases in the Transformer Residual Stream2023年3月16日</description>
    </item>
    <item>
      <title>Distributed Representations: Composition &amp; Superposition</title>
      <link>https://www.anthropic.com/research/distributed-representations-composition-superposition</link>
      <description>InterpretabilityDistributed Representations: Composition &amp; Superposition2023年5月4日</description>
    </item>
    <item>
      <title>Interpretability Dreams</title>
      <link>https://www.anthropic.com/research/interpretability-dreams</link>
      <description>InterpretabilityInterpretability Dreams2023年5月24日</description>
    </item>
    <item>
      <title>Circuits Updates — May 2023</title>
      <link>https://www.anthropic.com/research/circuits-updates-may-2023</link>
      <description>InterpretabilityCircuits Updates — May 20232023年5月24日</description>
    </item>
    <item>
      <title>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</title>
      <link>https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning</link>
      <description>InterpretabilityTowards Monosemanticity: Decomposing Language Models With Dictionary Learning2023年10月5日</description>
    </item>
    <item>
      <title>Decomposing Language Models Into Understandable Components</title>
      <link>https://www.anthropic.com/research/decomposing-language-models-into-understandable-components</link>
      <description>InterpretabilityDecomposing Language Models Into Understandable Components2023年10月5日</description>
    </item>
    <item>
      <title>Reflections on Qualitative Research</title>
      <link>https://www.anthropic.com/research/transformer-circuits</link>
      <description>InterpretabilityReflections on Qualitative Research2024年3月9日</description>
    </item>
    <item>
      <title>Simple probes can catch sleeper agents</title>
      <link>https://www.anthropic.com/research/probes-catch-sleeper-agents</link>
      <description>Alignment·  InterpretabilitySimple probes can catch sleeper agents2024年4月23日</description>
    </item>
    <item>
      <title>Circuits Updates – April 2024</title>
      <link>https://www.anthropic.com/research/circuits-updates-april-2024</link>
      <description>InterpretabilityCircuits Updates – April 20242024年4月27日</description>
    </item>
    <item>
      <title>Mapping the Mind of a Large Language Model</title>
      <link>https://www.anthropic.com/research/mapping-mind-language-model</link>
      <description>InterpretabilityMapping the Mind of a Large Language Model2024年5月21日</description>
    </item>
    <item>
      <title>The engineering challenges of scaling interpretability</title>
      <link>https://www.anthropic.com/research/engineering-challenges-interpretability</link>
      <description>InterpretabilityThe engineering challenges of scaling interpretability2024年6月13日</description>
    </item>
    <item>
      <title>Circuits Updates – June 2024</title>
      <link>https://www.anthropic.com/research/circuits-updates-june-2024</link>
      <description>InterpretabilityCircuits Updates – June 20242024年6月29日</description>
    </item>
    <item>
      <title>Circuits Updates – July 2024</title>
      <link>https://www.anthropic.com/research/circuits-updates-july-2024</link>
      <description>InterpretabilityCircuits Updates – July 20242024年8月1日</description>
    </item>
    <item>
      <title>Circuits Updates – August 2024</title>
      <link>https://www.anthropic.com/research/circuits-updates-august-2024</link>
      <description>InterpretabilityCircuits Updates – August 20242024年9月6日</description>
    </item>
    <item>
      <title>Circuits Updates – September 2024</title>
      <link>https://www.anthropic.com/research/circuits-updates-sept-2024</link>
      <description>InterpretabilityCircuits Updates – September 20242024年10月1日</description>
    </item>
    <item>
      <title>Using dictionary learning features as classifiers</title>
      <link>https://www.anthropic.com/research/features-as-classifiers</link>
      <description>InterpretabilityUsing dictionary learning features as classifiers2024年10月17日</description>
    </item>
    <item>
      <title>Evaluating feature steering: A case study in mitigating social biases</title>
      <link>https://www.anthropic.com/research/evaluating-feature-steering</link>
      <description>Societal Impacts·  InterpretabilityEvaluating feature steering: A case study in mitigating social biases2024年10月25日</description>
    </item>
    <item>
      <title>A General Language Assistant as a Laboratory for Alignment</title>
      <link>https://www.anthropic.com/research/a-general-language-assistant-as-a-laboratory-for-alignment</link>
      <description>AlignmentA General Language Assistant as a Laboratory for Alignment2021年12月1日</description>
    </item>
    <item>
      <title>A Mathematical Framework for Transformer Circuits</title>
      <link>https://www.anthropic.com/research/a-mathematical-framework-for-transformer-circuits</link>
      <description>InterpretabilityA Mathematical Framework for Transformer Circuits2021年12月22日</description>
    </item>
    <item>
      <title>Predictability and Surprise in Large Generative Models</title>
      <link>https://www.anthropic.com/research/predictability-and-surprise-in-large-generative-models</link>
      <description>Societal ImpactsPredictability and Surprise in Large Generative Models2022年2月15日</description>
    </item>
    <item>
      <title>In-context Learning and Induction Heads</title>
      <link>https://www.anthropic.com/research/in-context-learning-and-induction-heads</link>
      <description>InterpretabilityIn-context Learning and Induction Heads2022年3月8日</description>
    </item>
    <item>
      <title>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</title>
      <link>https://www.anthropic.com/research/training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback</link>
      <description>AlignmentTraining a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback2022年4月12日</description>
    </item>
    <item>
      <title>Scaling Laws and Interpretability of Learning from Repeated Data</title>
      <link>https://www.anthropic.com/research/scaling-laws-and-interpretability-of-learning-from-repeated-data</link>
      <description>InterpretabilityScaling Laws and Interpretability of Learning from Repeated Data2022年5月21日</description>
    </item>
    <item>
      <title>Softmax Linear Units</title>
      <link>https://www.anthropic.com/research/softmax-linear-units</link>
      <description>InterpretabilitySoftmax Linear Units2022年6月17日</description>
    </item>
    <item>
      <title>Language Models (Mostly) Know What They Know</title>
      <link>https://www.anthropic.com/research/language-models-mostly-know-what-they-know</link>
      <description>AlignmentLanguage Models (Mostly) Know What They Know2022年7月11日</description>
    </item>
    <item>
      <title>Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</title>
      <link>https://www.anthropic.com/research/red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned</link>
      <description>Societal ImpactsRed Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned2022年8月22日</description>
    </item>
    <item>
      <title>Toy Models of Superposition</title>
      <link>https://www.anthropic.com/research/toy-models-of-superposition</link>
      <description>InterpretabilityToy Models of Superposition2022年9月14日</description>
    </item>
    <item>
      <title>Measuring Progress on Scalable Oversight for Large Language Models</title>
      <link>https://www.anthropic.com/research/measuring-progress-on-scalable-oversight-for-large-language-models</link>
      <description>AlignmentMeasuring Progress on Scalable Oversight for Large Language Models2022年11月4日</description>
    </item>
    <item>
      <title>Constitutional AI: Harmlessness from AI Feedback</title>
      <link>https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback</link>
      <description>AlignmentConstitutional AI: Harmlessness from AI Feedback2022年12月15日</description>
    </item>
    <item>
      <title>Discovering Language Model Behaviors with Model-Written Evaluations</title>
      <link>https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</link>
      <description>AlignmentDiscovering Language Model Behaviors with Model-Written Evaluations2022年12月19日</description>
    </item>
    <item>
      <title>Superposition, Memorization, and Double Descent</title>
      <link>https://www.anthropic.com/research/superposition-memorization-and-double-descent</link>
      <description>InterpretabilitySuperposition, Memorization, and Double Descent2023年1月5日</description>
    </item>
    <item>
      <title>The Capacity for Moral Self-Correction in Large Language Models</title>
      <link>https://www.anthropic.com/research/the-capacity-for-moral-self-correction-in-large-language-models</link>
      <description>Societal ImpactsThe Capacity for Moral Self-Correction in Large Language Models2023年2月15日</description>
    </item>
    <item>
      <title>Privileged Bases in the Transformer Residual Stream</title>
      <link>https://www.anthropic.com/research/privileged-bases-in-the-transformer-residual-stream</link>
      <description>InterpretabilityPrivileged Bases in the Transformer Residual Stream2023年3月16日</description>
    </item>
    <item>
      <title>Distributed Representations: Composition &amp; Superposition</title>
      <link>https://www.anthropic.com/research/distributed-representations-composition-superposition</link>
      <description>InterpretabilityDistributed Representations: Composition &amp; Superposition2023年5月4日</description>
    </item>
    <item>
      <title>Interpretability Dreams</title>
      <link>https://www.anthropic.com/research/interpretability-dreams</link>
      <description>InterpretabilityInterpretability Dreams2023年5月24日</description>
    </item>
    <item>
      <title>Circuits Updates — May 2023</title>
      <link>https://www.anthropic.com/research/circuits-updates-may-2023</link>
      <description>InterpretabilityCircuits Updates — May 20232023年5月24日</description>
    </item>
    <item>
      <title>Towards Measuring the Representation of Subjective Global Opinions in Language Models</title>
      <link>https://www.anthropic.com/research/towards-measuring-the-representation-of-subjective-global-opinions-in-language-models</link>
      <description>Societal ImpactsTowards Measuring the Representation of Subjective Global Opinions in Language Models2023年6月29日</description>
    </item>
    <item>
      <title>Question Decomposition Improves the Faithfulness of Model-Generated Reasoning</title>
      <link>https://www.anthropic.com/research/question-decomposition-improves-the-faithfulness-of-model-generated-reasoning</link>
      <description>AlignmentQuestion Decomposition Improves the Faithfulness of Model-Generated Reasoning2023年7月18日</description>
    </item>
    <item>
      <title>Measuring Faithfulness in Chain-of-Thought Reasoning</title>
      <link>https://www.anthropic.com/research/measuring-faithfulness-in-chain-of-thought-reasoning</link>
      <description>AlignmentMeasuring Faithfulness in Chain-of-Thought Reasoning2023年7月18日</description>
    </item>
    <item>
      <title>Studying Large Language Model Generalization with Influence Functions</title>
      <link>https://www.anthropic.com/research/studying-large-language-model-generalization-with-influence-functions</link>
      <description>AlignmentStudying Large Language Model Generalization with Influence Functions2023年8月8日</description>
    </item>
    <item>
      <title>Tracing Model Outputs to the Training Data</title>
      <link>https://www.anthropic.com/research/influence-functions</link>
      <description>AlignmentTracing Model Outputs to the Training Data2023年8月8日</description>
    </item>
    <item>
      <title>Challenges in evaluating AI systems</title>
      <link>https://www.anthropic.com/research/evaluating-ai-systems</link>
      <description>PolicyChallenges in evaluating AI systems2023年10月4日</description>
    </item>
    <item>
      <title>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</title>
      <link>https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning</link>
      <description>InterpretabilityTowards Monosemanticity: Decomposing Language Models With Dictionary Learning2023年10月5日</description>
    </item>
    <item>
      <title>Decomposing Language Models Into Understandable Components</title>
      <link>https://www.anthropic.com/research/decomposing-language-models-into-understandable-components</link>
      <description>InterpretabilityDecomposing Language Models Into Understandable Components2023年10月5日</description>
    </item>
    <item>
      <title>Collective Constitutional AI: Aligning a Language Model with Public Input</title>
      <link>https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input</link>
      <description>Policy·  Societal ImpactsCollective Constitutional AI: Aligning a Language Model with Public Input2023年10月17日</description>
    </item>
    <item>
      <title>Towards Understanding Sycophancy in Language Models</title>
      <link>https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models</link>
      <description>AlignmentTowards Understanding Sycophancy in Language Models2023年10月24日</description>
    </item>
    <item>
      <title>Specific versus General Principles for Constitutional AI</title>
      <link>https://www.anthropic.com/research/specific-versus-general-principles-for-constitutional-ai</link>
      <description>AlignmentSpecific versus General Principles for Constitutional AI2023年10月25日</description>
    </item>
    <item>
      <title>Evaluating and Mitigating Discrimination in Language Model Decisions</title>
      <link>https://www.anthropic.com/research/evaluating-and-mitigating-discrimination-in-language-model-decisions</link>
      <description>Societal ImpactsEvaluating and Mitigating Discrimination in Language Model Decisions2023年12月8日</description>
    </item>
    <item>
      <title>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</title>
      <link>https://www.anthropic.com/research/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training</link>
      <description>AlignmentSleeper Agents: Training Deceptive LLMs that Persist Through Safety Training2024年1月15日</description>
    </item>
    <item>
      <title>Reflections on Qualitative Research</title>
      <link>https://www.anthropic.com/research/transformer-circuits</link>
      <description>InterpretabilityReflections on Qualitative Research2024年3月9日</description>
    </item>
    <item>
      <title>Many-shot jailbreaking</title>
      <link>https://www.anthropic.com/research/many-shot-jailbreaking</link>
      <description>AlignmentMany-shot jailbreaking2024年4月3日</description>
    </item>
    <item>
      <title>Measuring the Persuasiveness of Language Models</title>
      <link>https://www.anthropic.com/research/measuring-model-persuasiveness</link>
      <description>Societal ImpactsMeasuring the Persuasiveness of Language Models2024年4月9日</description>
    </item>
    <item>
      <title>Simple probes can catch sleeper agents</title>
      <link>https://www.anthropic.com/research/probes-catch-sleeper-agents</link>
      <description>Alignment·  InterpretabilitySimple probes can catch sleeper agents2024年4月23日</description>
    </item>
    <item>
      <title>Circuits Updates – April 2024</title>
      <link>https://www.anthropic.com/research/circuits-updates-april-2024</link>
      <description>InterpretabilityCircuits Updates – April 20242024年4月27日</description>
    </item>
    <item>
      <title>Mapping the Mind of a Large Language Model</title>
      <link>https://www.anthropic.com/research/mapping-mind-language-model</link>
      <description>InterpretabilityMapping the Mind of a Large Language Model2024年5月21日</description>
    </item>
    <item>
      <title>Testing and mitigating elections-related risks</title>
      <link>https://www.anthropic.com/news/testing-and-mitigating-elections-related-risks</link>
      <description>Policy·  Societal ImpactsTesting and mitigating elections-related risks2024年6月6日</description>
    </item>
    <item>
      <title>Claude’s Character</title>
      <link>https://www.anthropic.com/research/claude-character</link>
      <description>AlignmentClaude’s Character2024年6月9日</description>
    </item>
    <item>
      <title>The engineering challenges of scaling interpretability</title>
      <link>https://www.anthropic.com/research/engineering-challenges-interpretability</link>
      <description>InterpretabilityThe engineering challenges of scaling interpretability2024年6月13日</description>
    </item>
    <item>
      <title>Sycophancy to subterfuge: Investigating reward tampering in language models</title>
      <link>https://www.anthropic.com/research/reward-tampering</link>
      <description>AlignmentSycophancy to subterfuge: Investigating reward tampering in language models2024年6月17日</description>
    </item>
    <item>
      <title>Circuits Updates – June 2024</title>
      <link>https://www.anthropic.com/research/circuits-updates-june-2024</link>
      <description>InterpretabilityCircuits Updates – June 20242024年6月29日</description>
    </item>
    <item>
      <title>Circuits Updates – July 2024</title>
      <link>https://www.anthropic.com/research/circuits-updates-july-2024</link>
      <description>InterpretabilityCircuits Updates – July 20242024年8月1日</description>
    </item>
    <item>
      <title>Circuits Updates – August 2024</title>
      <link>https://www.anthropic.com/research/circuits-updates-august-2024</link>
      <description>InterpretabilityCircuits Updates – August 20242024年9月6日</description>
    </item>
    <item>
      <title>Circuits Updates – September 2024</title>
      <link>https://www.anthropic.com/research/circuits-updates-sept-2024</link>
      <description>InterpretabilityCircuits Updates – September 20242024年10月1日</description>
    </item>
    <item>
      <title>Using dictionary learning features as classifiers</title>
      <link>https://www.anthropic.com/research/features-as-classifiers</link>
      <description>InterpretabilityUsing dictionary learning features as classifiers2024年10月17日</description>
    </item>
    <item>
      <title>Sabotage evaluations for frontier models</title>
      <link>https://www.anthropic.com/research/sabotage-evaluations</link>
      <description>AlignmentSabotage evaluations for frontier models2024年10月19日</description>
    </item>
    <item>
      <title>Developing a computer use model</title>
      <link>https://www.anthropic.com/news/developing-computer-use</link>
      <description>Announcements·  ProductDeveloping a computer use model2024年10月23日</description>
    </item>
    <item>
      <title>Evaluating feature steering: A case study in mitigating social biases</title>
      <link>https://www.anthropic.com/research/evaluating-feature-steering</link>
      <description>Societal Impacts·  InterpretabilityEvaluating feature steering: A case study in mitigating social biases2024年10月25日</description>
    </item>
    <item>
      <title>Raising the bar on SWE-bench Verified with Claude 3.5 Sonnet</title>
      <link>https://www.anthropic.com/research/swe-bench-sonnet</link>
      <description>ProductRaising the bar on SWE-bench Verified with Claude 3.5 Sonnet2024年10月31日</description>
    </item>
    <item>
      <title>A statistical approach to model evaluations</title>
      <link>https://www.anthropic.com/research/statistical-approach-to-model-evals</link>
      <description>EvaluationsA statistical approach to model evaluations2024年11月20日</description>
    </item>
    <item>
      <title>Clio: A system for privacy-preserving insights into real-world AI use</title>
      <link>https://www.anthropic.com/research/clio</link>
      <description>Societal ImpactsClio: A system for privacy-preserving insights into real-world AI use2024年12月12日</description>
    </item>
    <item>
      <title>Alignment faking in large language models</title>
      <link>https://www.anthropic.com/research/alignment-faking</link>
      <description>AlignmentAlignment faking in large language models2024年12月18日</description>
    </item>
    <item>
      <title>Building effective agents</title>
      <link>https://www.anthropic.com/research/building-effective-agents</link>
      <description>ProductBuilding effective agents2024年12月20日</description>
    </item>
  </channel>
</rss>
